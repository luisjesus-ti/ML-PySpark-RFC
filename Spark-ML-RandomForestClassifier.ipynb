{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb3af1ec-a2fb-4f76-bf86-2ed3e7bbbd9e",
   "metadata": {},
   "source": [
    "# Artigo no Linkedin\n",
    "\n",
    "## Classificação usando Spark - RandonForestClassifier\n",
    "\n",
    "### Dados\n",
    "\n",
    "Dados de Acidentes da Polícia Rodoviária Federal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc7f95b-3cde-41d9-935d-faff1d993556",
   "metadata": {},
   "source": [
    "# 1. INICIALIZAÇÃO DO NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d52f91-dfda-473d-8bf3-23200d4c6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#  Imports\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Utilidades\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "# Importar o PySpark\n",
    "import pyspark\n",
    "\n",
    "# pyspark machine learning\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# pyspark SQL\n",
    "from pyspark.sql.functions import when, col, trim, countDistinct, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, DoubleType, LongType\n",
    "\n",
    "# import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# retirar mensagens de warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e27672-dc13-4749-8d3e-ecb75b7dbe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Sessão Pyspark - SparkSession\n",
    "\n",
    "# Sessão\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[4]') \\\n",
    "    .appName(\"ClassifierCrash\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1566c2-29b3-40c2-88e8-617e8462a855",
   "metadata": {},
   "source": [
    "# 2. TRATAMENTO DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457dc8e6-4ac0-452f-8119-422dff9562f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Quantidade de anos de dados de acidentes a serem processados\n",
    "# 1 = 2016 | 2 = 2016 e 2017 | ... | 7 = 2016 até 2021\n",
    "\n",
    "qtd_anos_processamento = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b911f4-9a22-43b5-a6bf-4cc22185c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "#  Definição do Schema - Campos dos CSVs que serão carregados\n",
    "# Observação: target não é campo do CSV\n",
    "\n",
    "acidente_schema = StructType([\n",
    "         StructField(\"id\",IntegerType(),True),\n",
    "         StructField(\"data_inversa\",StringType(),True),\n",
    "         StructField(\"dia_semana\",StringType(),True),\n",
    "         StructField(\"horario\",StringType(),True),\n",
    "         StructField(\"uf\",StringType(),True),\n",
    "         StructField(\"br\",IntegerType(),True),\n",
    "         StructField(\"km\",StringType(),True),\n",
    "         StructField(\"municipio\",StringType(),True),\n",
    "         StructField(\"causa_acidente\",StringType(),True),\n",
    "         StructField(\"tipo_acidente\",StringType(),True),\n",
    "         StructField(\"classificacao_acidente\",StringType(),True),\n",
    "         StructField(\"fase_dia\",StringType(),True),\n",
    "         StructField(\"sentido_via\",StringType(),True),\n",
    "         StructField(\"condicao_metereologica\",StringType(),True),\n",
    "         StructField(\"tipo_pista\",StringType(),True),\n",
    "         StructField(\"tracado_via\",StringType(),True),\n",
    "         StructField(\"uso_solo\",StringType(),True),\n",
    "         StructField(\"pessoas\",IntegerType(),True),\n",
    "         StructField(\"mortos\",IntegerType(),True),\n",
    "         StructField(\"feridos_leves\",IntegerType(),True),\n",
    "         StructField(\"feridos_graves\",IntegerType(),True),\n",
    "         StructField(\"ilesos\",IntegerType(),True),\n",
    "         StructField(\"ignorados\",IntegerType(),True),\n",
    "         StructField(\"feridos\",IntegerType(),True),\n",
    "         StructField(\"veiculos\",IntegerType(),True),\n",
    "         StructField(\"target\",IntegerType(),True)\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9df24-8aaf-401d-9cd2-ccdb23ecdf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "#  Procedures e Funções\n",
    "\n",
    "def _carrega_spark_dataframe(_ano, df=None, mySchema=None, _separador=\",\", _enconding=\"latin1\"):\n",
    "    print(f\"Início da carga do arquivo de acidentes de {_ano}....\", datetime.today())\n",
    "    \n",
    "    # Carregar o arquivo para o spark dataframe\n",
    "    dftmp = spark.read.format(\"csv\").schema(mySchema).option(\"header\",\"True\").option(\"sep\",_separador).option(\"encoding\",_enconding).load(f\"./dados/datatran{_ano}.csv\")\n",
    "    # Verificar se foi passado dataframe\n",
    "    if df==None:\n",
    "        df = dftmp\n",
    "    else:\n",
    "        df = df.union(dftmp)\n",
    "    \n",
    "    # print após carga\n",
    "    print(f\"Fim da carga do arquivo de acidentes de {_ano}....\", datetime.today())\n",
    "    print(\"Total de registros carregados...\",dftmp.count())\n",
    "    print(\"Total de registros acumulados...\",df.count())\n",
    "    # delete de dataframe temporário\n",
    "    del dftmp\n",
    "    # retornar o dataframe concatenado\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4397618-aa38-4ee3-99ec-2bac9ac7a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Realização da carga do arquivos para dataframe\n",
    "# parâmetros: ano dos regitros, dataframe, separador, encoding\n",
    "\n",
    "if qtd_anos_processamento >= 1:\n",
    "    dft = _carrega_spark_dataframe(\"2016\", None, acidente_schema, \";\",\"latin1\")\n",
    "if qtd_anos_processamento >= 2:\n",
    "    dft = _carrega_spark_dataframe(\"2017\", dft, acidente_schema, \";\",\"latin1\")\n",
    "if qtd_anos_processamento >= 3:\n",
    "    dft = _carrega_spark_dataframe(\"2018\", dft, acidente_schema, \";\",\"latin1\")\n",
    "if qtd_anos_processamento >= 4:\n",
    "    dft = _carrega_spark_dataframe(\"2019\", dft, acidente_schema, \";\",\"latin1\")\n",
    "if qtd_anos_processamento >= 5:\n",
    "    dft = _carrega_spark_dataframe(\"2020\", dft, acidente_schema, \";\",\"latin1\")\n",
    "if qtd_anos_processamento >= 6:\n",
    "    dft = _carrega_spark_dataframe(\"2021\", dft, acidente_schema, \";\",\"latin1\")\n",
    "if qtd_anos_processamento >= 7:\n",
    "    dft = _carrega_spark_dataframe(\"2022\", dft, acidente_schema, \";\",\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de42f52-81fc-491c-bb6c-5ebc00673395",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Copiar DataFrame\n",
    "\n",
    "sparkDF = dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0a17d-2875-4cff-8172-c5db25c0cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "#  Atualizar campo target\n",
    "\n",
    "# Marcar a coluna target - 1 = Acidente Grave | 2 = Acidente não grave\n",
    "sparkDF = sparkDF.withColumn(\"target\", when(sparkDF.mortos >= 1, 1) \\\n",
    "      .when(sparkDF.feridos_graves >=1, 1) \\\n",
    "      .otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a825fb-2eaa-480b-b5b5-6132c5961880",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Retirar os campos com colunas vazias - usar o na.drop()\n",
    "\n",
    "print(\"Retirada de registros que tem campos nulos ....\")\n",
    "print(\"Total de registros no Dataframe antes da limpeza = \", sparkDF.count())\n",
    "sparkDF = sparkDF.na.drop()\n",
    "print(\"Total de registros no Dataframe após a limpeza = \", sparkDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438ed67-133a-41d6-b9c4-06031291b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Retirar registros que não farão parte da classificação\n",
    "# Deixar somente os registros de acidentes com vítimas\n",
    "\n",
    "# Filtrar\n",
    "\n",
    "print(\"Retirada de registros de acidentes sem vítimas e ignorados ....\")\n",
    "print(\"Total de registros no Dataframe antes da limpeza = \", sparkDF.count())\n",
    "sparkDF = sparkDF.filter(col(\"classificacao_acidente\") != 'Ignorados')\n",
    "print(\"Total de registros no Dataframe após a limpeza de 'Ignorados' = \", sparkDF.count())\n",
    "\n",
    "sparkDF = sparkDF.filter(col(\"classificacao_acidente\") != 'Sem Vítimas')\n",
    "print(\"Total de registros no Dataframe após a limpeza de 'Sem Vítimas' = \", sparkDF.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9770b18f-0cf7-4967-93c0-3b55b4a50f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Colunas categóricas - Lista das colunas\n",
    "\n",
    "categoricalColumns = [ \"dia_semana\"\n",
    "                      ,\"causa_acidente\"\n",
    "                      ,\"tipo_acidente\"\n",
    "                      ,\"classificacao_acidente\"\n",
    "                      ,\"fase_dia\"\n",
    "                      ,\"sentido_via\"\n",
    "                      ,\"condicao_metereologica\"\n",
    "                      ,\"tipo_pista\"\n",
    "                      ,\"tracado_via\"\n",
    "                      ,\"uso_solo\"\n",
    "                      ,\"pessoas\"\n",
    "                      ,\"veiculos\"\n",
    "                     ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5bfce4-03f0-4883-b852-a59b586a3c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Encode dos dados das Colunas categóricas\n",
    "\n",
    "# loop \n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol+\"_encoded\").fit(sparkDF)\n",
    "    sparkDF = stringIndexer.transform(sparkDF)\n",
    "    sparkDF = sparkDF.withColumn(categoricalCol+\"_encoded\", sparkDF[categoricalCol+\"_encoded\"].cast('int'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c9fb5-51f3-4115-b288-4ec2a975eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Print do Schema do dataframe\n",
    "\n",
    "sparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0b681-389c-4852-8faf-25a6c5370bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Criar DataFrame com enconded\n",
    "\n",
    "encoded_df =  sparkDF.select(\"dia_semana_encoded\"\n",
    "                      ,\"causa_acidente_encoded\"\n",
    "                      ,\"tipo_acidente_encoded\"\n",
    "                      ,\"classificacao_acidente_encoded\"\n",
    "                      ,\"fase_dia_encoded\"\n",
    "                      ,\"sentido_via_encoded\"\n",
    "                      ,\"condicao_metereologica_encoded\"\n",
    "                      ,\"tipo_pista_encoded\"\n",
    "                      ,\"tracado_via_encoded\"\n",
    "                      ,\"uso_solo_encoded\"\n",
    "                      ,\"pessoas_encoded\"\n",
    "                      ,\"veiculos_encoded\"\n",
    "                      ,\"target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31500ba6-3860-4728-99e2-ab08dff16aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Preparar features extraction\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featureAssembler = VectorAssembler(inputCols=[\"dia_semana_encoded\"\n",
    "                      ,\"causa_acidente_encoded\"\n",
    "                      ,\"tipo_acidente_encoded\"\n",
    "                      ,\"classificacao_acidente_encoded\"\n",
    "                      ,\"fase_dia_encoded\"\n",
    "                      ,\"sentido_via_encoded\"\n",
    "                      ,\"condicao_metereologica_encoded\"\n",
    "                      ,\"tipo_pista_encoded\"\n",
    "                      ,\"tracado_via_encoded\"\n",
    "                      ,\"uso_solo_encoded\"\n",
    "                      ,\"pessoas_encoded\"\n",
    "                      ,\"veiculos_encoded\"\n",
    "                                             ],outputCol=\"features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b22de25-07a5-4e86-898d-2a695a7c6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Assembler \n",
    "\n",
    "output = featureAssembler.transform(encoded_df)\n",
    "\n",
    "output.withColumnRenamed(\"target\",\"labels\").printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6bda7d-b5d0-4f4b-b1aa-f75a7a1cbfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Mostrar o resultado do assembler \n",
    "\n",
    "output.select(\"features\",\"target\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e68e0f-37a2-49d6-bc84-246ad585bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Preparação labels \n",
    "\n",
    "udf_result = StructType([StructField('target',IntegerType())])\n",
    "\n",
    "target_dict = {'Não Grave': '0', 'Grave': '1'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04cd8be-878d-49f4-89bc-7061ca95462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# função\n",
    "def assign_labels(target):\n",
    "    return Row(target_dict[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a4eb0-ae26-4363-a04b-64de109f0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#assign_labels_udf = F.udf(assign_labels, udf_result)\n",
    "assign_labels_udf = udf(assign_labels, udf_result)\n",
    "\n",
    "output.withColumn('labels', assign_labels_udf('target')).drop('target').printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e5fb7-94e0-4c09-a8a3-74f47ca5cccd",
   "metadata": {},
   "source": [
    "# 3. SPARK - RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf53622-878a-4e77-b4ff-13a308399941",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Separar em treino e teste\n",
    "\n",
    "# Quantidade de rodadas na variável m\n",
    "m = 5\n",
    "\n",
    "# inicialização de variáveis \n",
    "resultado = []\n",
    "l_start_fit_spark = []\n",
    "l_stop_fit_spark = []\n",
    "l_start_predict_spark = []\n",
    "l_stop_predict_spark = []\n",
    "l_acuracia = []\n",
    "l_total_registros = []\n",
    "l_rodada = []\n",
    "\n",
    "# Quantidade de registros no processamento\n",
    "total_registros = sparkDF.count()\n",
    "\n",
    "# classificador com parâmetros básicos\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'target')\n",
    "\n",
    "# loop repetindo o total de registros para avaliar o tempo médio de execução e acurária\n",
    "for i in range(m):\n",
    "    # split\n",
    "    train, test = output.randomSplit([0.7, 0.3])\n",
    "    \n",
    "    #train.show(5)\n",
    "    \n",
    "    #Training Model\n",
    "    start_fit_spark =  datetime.today()\n",
    "    print(\"Start : RandomForestClassifier ....\",start_fit_spark)\n",
    "    rfModel = rf.fit(train)\n",
    "    stop_fit_spark =  datetime.today()\n",
    "    print(\"Stop  : RandomForestClassifier ....\", datetime.today())\n",
    "\n",
    "    \n",
    "    #Prediction\n",
    "    start_predict_spark =  datetime.today()\n",
    "    print(\"Start : RandomForestClassifier Transform ....\",start_predict_spark)\n",
    "    predictions = rfModel.transform(test)\n",
    "    stop_predict_spark =  datetime.today() \n",
    "    print(\"Stop  : RandomForestClassifier Transform ....\",stop_predict_spark)\n",
    "    \n",
    "    \n",
    "    #Avaliação da performance\n",
    "    evaluator = MulticlassClassificationEvaluator()\n",
    "    evaluator.setLabelCol(\"target\")\n",
    "    evaluator.setPredictionCol(\"prediction\")\n",
    "    acucacia = evaluator.evaluate(predictions)\n",
    "\n",
    "    # guardar resultado\n",
    "    l_start_fit_spark.append(start_fit_spark)\n",
    "    l_stop_fit_spark.append(stop_fit_spark)\n",
    "    l_start_predict_spark.append(start_predict_spark)\n",
    "    l_stop_predict_spark.append(stop_predict_spark)\n",
    "    l_acuracia.append(acucacia)\n",
    "    l_total_registros.append(total_registros)\n",
    "    l_rodada.append(qtd_anos_processamento)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88198ead-8d8f-41cb-a0c9-c076c189b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Cria dataframe com os resultados do processamento de loop\n",
    "\n",
    "df_resultado = pd.DataFrame(zip(l_rodada, l_total_registros, l_start_fit_spark, l_stop_fit_spark, l_start_predict_spark, l_stop_predict_spark, l_acuracia),\n",
    "                            columns = ['rodada','total_registros', 'start_fit', 'stop_fit', 'start_predict', 'stop_predict', 'acuracia'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b58f2e-395c-47c9-8eaf-29e74ffecf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Criar campo com o tempo de processamento\n",
    "\n",
    "df_resultado['tempo_fit'] = df_resultado['stop_fit'] - df_resultado['start_fit']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c89b7-a77b-40ac-ba83-899bd530cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####################################\n",
    "# Imprimir resultado\n",
    "\n",
    "df_resultado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
